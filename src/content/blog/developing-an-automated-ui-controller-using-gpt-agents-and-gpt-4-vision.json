{
  "id": "developing_an_automated_ui_controller_using_gpt_ag",
  "slug": "developing-an-automated-ui-controller-using-gpt-agents-and-gpt-4-vision",
  "title": "Developing an Automated UI Controller using GPT Agents and GPT-4 Vision",
  "excerpt": "The emergence of Large Language Models (LLMs) like ChatGPT has ushered in a new era in text generation and AI advancements. While tools such asAutoGP",
  "content": "<h2 id=\">1. Introduction</h2>\n\n<p id=\">The emergence of Large Language Models (LLMs) like ChatGPT has ushered in a new era in text generation and AI advancements. While tools such as AutoGPT have aimed at task automation, their reliance on underlying DOM/HTML code poses challenges for desktop applications built on .NET/.SAP. The GPT-4 Vision (GPT-4v) API addresses this limitation by focusing on visual inputs, eliminating the need for HTML code.</p><p id=\"><a href=\"https://medium.com/@gitlostmurali/creating-an-automated-ui-controller-with-gpt-agents-35340759d08b\" id=\">See the original post here</a></p><h3 id=\">1.1 The Challenge of Grounding</h3>\n\n<p id=\">Although GPT-4v excels in image analysis, accurately identifying UI element locations remains a challenge. In response, a solution called \"Grounding\" has been introduced, involving pre-annotating images with tools like Segment-Anything (SAM) to simplify object identification for GPT-4v.</p><figure id=\" class=\"w-richtext-figure-type-image w-richtext-align-fullwidth\" style=\"max-width:700px\" data-rt-type=\"image\" data-rt-align=\"fullwidth\" data-rt-max-width=\"700px\"><div id=\"><img src=\"https://cdn.prod.website-files.com/6630f90ff7431b0c5b1bb0e7/6634d1b8c39f787e903e8604_6591831a7b1ba38a57bb1984_1*qhmIt7p1x188riqcwN6X9Q.png\" id=\" width=\"auto\" height=\"auto\" alt=\" loading=\"auto\"></div><figcaption id=\">GPT-4 Vision model struggling to locate people correctly. (Source: <a href=\"https://huyenchip.com/2023/10/10/multimodal.html\" target=\"_blank\" id=\">https://huyenchip.com/2023/10/10/multimodal.html</a>)</figcaption></figure><figure id=\" class=\"w-richtext-figure-type-image w-richtext-align-fullwidth\" style=\"max-width:700px\" data-rt-type=\"image\" data-rt-align=\"fullwidth\" data-rt-max-width=\"700px\"><div id=\"><img src=\"https://cdn.prod.website-files.com/6630f90ff7431b0c5b1bb0e7/6634d1b8c39f787e903e8601_6591838fd0f6576232e6ee9e_1*gOuWsXp3t9g0KI35XLNsrQ.png\" id=\" width=\"auto\" height=\"auto\" alt=\" loading=\"auto\"></div><figcaption id=\">Set of Mark example</figcaption></figure><h3 id=\">1.2 Adapting to the Usecase</h3>\n\n<p id=\">UI elements can be numbered for reference, streamlining the interaction process. For example, the \"Start for Free\" button might be associated with the number \"34,\" enabling GPT-4v to provide instructions that can be translated into corresponding coordinates for controller actions.</p><figure id=\" class=\"w-richtext-figure-type-image w-richtext-align-fullwidth\" style=\"max-width:700px\" data-rt-type=\"image\" data-rt-align=\"fullwidth\" data-rt-max-width=\"700px\"><div id=\"><img src=\"https://cdn.prod.website-files.com/6630f90ff7431b0c5b1bb0e7/6634d1b8c39f787e903e85f8_659183a516ca683d432ef143_1*00p-__SbrNDQPl8-RSSMqQ.png\" id=\" width=\"auto\" height=\"auto\" alt=\" loading=\"auto\"></div><figcaption id=\">Object Detection on a UI screen</figcaption></figure><h2 id=\">2. Building Agents</h2>\n\n<p id=\">The envisioned product/tool aims to enter a goal/task and receive a set of actions to achieve it. The system operates in three sequential capabilities: goal breakdown, vision (image understanding), and system control. A GPT-4 text model acts as the orchestrator, interfacing with both GPT-4v and the device controller.</p><figure id=\" class=\"w-richtext-figure-type-image w-richtext-align-fullwidth\" style=\"max-width:700px\" data-rt-type=\"image\" data-rt-align=\"fullwidth\" data-rt-max-width=\"700px\"><div id=\"><img src=\"https://cdn.prod.website-files.com/6630f90ff7431b0c5b1bb0e7/6634d1b8c39f787e903e85f5_659183ee7b71a007811ecad1_1*xTwwxGj_denY38sYfKoTNw.jpeg\" id=\" width=\"auto\" height=\"auto\" alt=\" loading=\"auto\"></div><figcaption id=\">UI controller agent which can plan and analyze images</figcaption></figure><h2 id=\">3. GPT-4V Integration: Visionary Function</h2>\n\n<p id=\">GPT-4v serves as the visual interpreter, analyzing UI screenshots and identifying elements for interaction. An example function is provided to showcase how UI element identification is requested</p><pre></pre><h2 id=\">4. Device Controller: The Interactive Core</h2>\n\n<p id=\">The Controller class is pivotal, executing UI actions based on GPT-4v's guidance. Functions include moving the mouse, double-clicking, entering text, and capturing annotated screenshots, ensuring seamless interaction with the system:</p><p id=\"><strong id=\">- move_mouse():</strong> Moves the cursor and performs clicks.</p><p id=\"><strong id=\">- double_click_at_location():</strong> Executes double clicks at specified locations.</p><p id=\"><strong id=\">- enter_text_at_location()</strong>: Inputs text at a given location.</p><p id=\"><strong id=\">- take_screenshot():</strong> Captures and annotates the current UI state.</p><p id=\">The below class ensures seamless interaction with the system, acting based on GPT-4v's guidance.</p><pre></pre><h2 id=\">5. Orchestrator: The Strategic Planner</h2>\n\n<p id=\">The Orchestrator, a GPT-4 text model, plans and executes tasks by leveraging the capabilities of the Device Controller and GPT-4v. Key components include the Planner (goal breakdown and action strategy), UserProxyAgent (communication facilitator), and Controller (UI action execution).</p><h3 id=\">Key Components</h3><ul id=\"><li id=\"><strong id=\">Planner</strong>: Breaks down goals and strategizes actions.</li></ul><ul id=\"><li id=\"><strong id=\">UserProxyAgent</strong>: Facilitates communication between the planner and the controller.</li></ul><ul id=\"><li id=\"><strong id=\">Controller</strong>: Executes actions within the UI.</li></ul><h3 id=\">Workflow Example</h3>\n\n<p id=\"><strong id=\">1. Initialization</strong>: The Orchestrator receives a task.</p><p id=\"><strong id=\">2. Planning</strong>: It outlines the necessary steps.<strong id=\">‍</strong></p><p id=\"><strong id=\">3. Execution</strong>: The Controller interacts with the UI, guided by the Orchestrator’s strategy.</p><p id=\"><strong id=\">4. Verification</strong>: The Orchestrator checks the outcomes and adjusts actions if needed.</p><p id=\">We use <a href=\"https://github.com/microsoft/autogen\" target=\"_blank\" id=\"><strong id=\">AutoGen</strong></a>, an open source library that lets GPTs talk to each other, to implement the Automated UI controller. After giving each GPT its system message (or identity), we register the functions so that the GPT is aware of them and can call them when needed.</p><pre></pre><h2 id=\">6. Workflow on a Sample Task</h2>\n\n<p id=\">The workflow involves the Orchestrator receiving a task, planning the necessary steps, executing actions through the Controller, and verifying outcomes. The article demonstrates a sample task—\"click on the GitHub icon and click on the 'blogs' repository\"—with a log history showcasing various executed functions.</p><figure id=\" class=\"w-richtext-figure-type-video w-richtext-align-fullwidth\" style=\"padding-bottom:56.206088992974244%\" data-rt-type=\"video\" data-rt-align=\"fullwidth\" data-rt-max-width=\" data-rt-max-height=\"56.206088992974244%\" data-rt-dimensions=\"854:480\" data-page-url=\"https://www.youtube.com/watch?v=-9adrW2FKac\"><div id=\"><iframe allowfullscreen=\"true\" frameborder=\"0\" scrolling=\"no\" src=\"https://www.youtube.com/embed/-9adrW2FKac\" title=\"AutoGPT with GPT-4 Vision\"></iframe></div></figure><p id=\">Attached below is a log history, performing various actions. We can see that various functions are called and executed accordingly.</p><figure id=\" class=\"w-richtext-figure-type-image w-richtext-align-fullwidth\" style=\"max-width:700px\" data-rt-type=\"image\" data-rt-align=\"fullwidth\" data-rt-max-width=\"700px\"><div id=\"><img src=\"https://cdn.prod.website-files.com/6630f90ff7431b0c5b1bb0e7/6634d1b8c39f787e903e85fc_659185f323cc8cfff0acfa34_1*JuTTwFN4iue0A78cCxC-kQ.png\" id=\" width=\"auto\" height=\"auto\" alt=\" loading=\"auto\"></div></figure><p id=\">This article provides insight into the evolving automation landscape, anticipating remarkable developments built upon these technologies.</p>",
  "category": "Academy",
  "readTime": "3 min read",
  "date": "2024-11-11",
  "publishedAt": "Mon Nov 11 2024 11:18:13 GMT+0000 (Coordinated Universal Time)",
  "author": "murali-kondragunta",
  "image": "https://cdn.prod.website-files.com/6630f90ff7431b0c5b1bb0e7/6634d1b8c39f787e903e85e9_659186107c3346c02afe0b94_1_xTwwxGj_denY38sYfKoTNw.webp",
  "featured": false,
  "contentPath": "developing-an-automated-ui-controller-using-gpt-agents-and-gpt-4-vision.md"
}